{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chardet\n",
    "#!pip install dawg\n",
    "#!pip install gensim\n",
    "#!pip install pattern\n",
    "#!pip install razdel\n",
    "#!pip install pymorphy2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "os.chdir(r'D:\\INSTALL\\Python3\\PROJECTS\\REPOS\\nlp_toolkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'D:\\INSTALL\\Python3\\PROJECTS\\REPOS\\nlp_toolkit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlptk\n",
    "from nlptk.mining.text import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Prep in module nlptk.mining.text object:\n",
      "\n",
      "class Prep(nlptk.misc.mixins.TokenizerMixin, nlptk.misc.mixins.LemmatizerMixin, nlptk.misc.mixins.SentencizerMixin, nlptk.misc.mixins.TaggerMixin)\n",
      " |  Prep(*args, **kwargs)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Prep\n",
      " |      nlptk.misc.mixins.TokenizerMixin\n",
      " |      nlptk.misc.mixins.LemmatizerMixin\n",
      " |      nlptk.misc.mixins.SentencizerMixin\n",
      " |      nlptk.misc.mixins.TaggerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from nlptk.misc.mixins.TokenizerMixin:\n",
      " |  \n",
      " |  nonalpha_tokenize(text)\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York.  Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> RE_NONALPHA.split(s)\n",
      " |              ['Good', 'muffins', 'cost', '', '3', '88', 'in', 'New', 'York', \n",
      " |              '', '', 'Please', 'buy', 'me', 'two', 'of', 'them', '', '', 'Thanks', '']\n",
      " |  \n",
      " |  punct_tokenize(text)\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> RE_PUNCT.split(s)\n",
      " |              ['Good muffins cost ', '$', '3', '.', '88\n",
      " |      in New York', '.', \n",
      " |              ' It', \"'\", 's inexpensive', '.', ' Free', '-', 'for', '-', 'all', '.', \n",
      " |              ' Please buy me\n",
      " |      two of them', '.', '\n",
      " |      \n",
      " |      Thanks', '.', '']\n",
      " |  \n",
      " |  regexp_tokenize(text, pattern=None)\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
      " |              >>> tokenizer.tokenize(s)\n",
      " |              ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', \n",
      " |              'It', \"'s\", 'inexpensive', '.', 'Free', '-for-all.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      " |  \n",
      " |  simple_tokenize(text)\n",
      " |      Tokenize input test using gensim.utils.PAT_ALPHABETIC.\n",
      " |              Using regexp (((?![\\d])\\w)+)\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York.  Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> list(gensim.utils.simple_tokenize(s))\n",
      " |              ['Good', 'muffins', 'cost', 'in', 'New', 'York', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n",
      " |  \n",
      " |  simple_tokenize2(text)\n",
      " |              Using regexp \\w+'\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York.  Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> RE_WORD.findall(s)\n",
      " |              ['Good', 'muffins', 'cost', '3', '88', 'in', 'New', 'York', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n",
      " |  \n",
      " |  space_tokenizer(text)\n",
      " |      Only \" \" blank character\n",
      " |              Same as s.split(\" \")\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> SpaceTokenizer().tokenize(s)\n",
      " |              ['Good', 'muffins', 'cost', '$3.88\n",
      " |      in', 'New', 'York.', \n",
      " |              \"It's\", 'inexpensive.', 'Free-for-all.', \n",
      " |              'Please', 'buy', 'me\n",
      " |      two', 'of', 'them.\n",
      " |      \n",
      " |      Thanks.']\n",
      " |              >>> s.split(' ')\n",
      " |              ['Good', 'muffins', 'cost', '$3.88\n",
      " |      in', 'New', 'York.', \n",
      " |              \"It's\", 'inexpensive.', 'Free-for-all.', \n",
      " |              'Please', 'buy', 'me\n",
      " |      two', 'of', 'them.\n",
      " |      \n",
      " |      Thanks.']\n",
      " |              >>>\n",
      " |  \n",
      " |  tab_tokenizer(text)\n",
      " |      Tab\n",
      " |  \n",
      " |  tags_tokenize(text)\n",
      " |  \n",
      " |  token_tokenize(text)\n",
      " |  \n",
      " |  toktok_tokenize(text)\n",
      " |              >>> text = u'Is 9.5 or 525,600 my favorite number?'\n",
      " |              >>> ToktokTokenizer().tokenize(text)\n",
      " |              ['Is', '9.5', 'or', '525,600', 'my', 'favorite', 'number', '?']\n",
      " |              s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> ToktokTokenizer().tokenize(s)\n",
      " |              ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', \n",
      " |              'It', \"'\", 's', 'inexpensive.', 'Free-for-all.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n",
      " |              >>>\n",
      " |  \n",
      " |  treebank_word_tokenize(text)\n",
      " |              using NLTK’s recommended word tokenizer (currently an improved \n",
      " |              TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language)\n",
      " |              \n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> nltk.word_tokenize(s)\n",
      " |              ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', \n",
      " |              'It', \"'s\", 'inexpensive', '.', 'Free-for-all', '.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      " |  \n",
      " |  whitespace_tokenize2(text)\n",
      " |      >>> s = \"Good muffins cost $3.88\n",
      " |      in New York.  Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> RE_WHITESPACE.split(s)\n",
      " |              ['Good', ' ', 'muffins', ' ', 'cost', ' ', '$3.88', '\n",
      " |      ', 'in', ' ', \n",
      " |              'New', ' ', 'York.', ' ', 'Please', ' ', 'buy', ' ', 'me', '\n",
      " |      ', \n",
      " |              'two', ' ', 'of', ' ', 'them.', '\n",
      " |      ', 'Thanks.']\n",
      " |  \n",
      " |  whitespace_tokenizer(text)\n",
      " |      space, tab, newline\n",
      " |              Same as s.split()\n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', \n",
      " |              \"It's\", 'inexpensive.', 'Free-for-all.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n",
      " |              >>>\n",
      " |              >>> s.split()\n",
      " |              ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', \n",
      " |              \"It's\", 'inexpensive.', 'Free-for-all.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n",
      " |              >>>\n",
      " |  \n",
      " |  wordpunct_tokenize(text)\n",
      " |              Using the regexp \\w+|[^\\w\\s]+\n",
      " |              \n",
      " |              >>> s = \"Good muffins cost $3.88\n",
      " |      in New York. It's inexpensive. Free-for-all. Please buy me\n",
      " |      two of them.\n",
      " |      \n",
      " |      Thanks.\"\n",
      " |              >>> WordPunctTokenizer().tokenize(s)\n",
      " |              ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', \n",
      " |              'It', \"'\", 's', 'inexpensive', '.', 'Free', '-', 'for', '-', 'all', '.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      " |              >>> \n",
      " |              \n",
      " |              >>> nltk.tokenize.word_tokenize(s)\n",
      " |              ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', \n",
      " |              'It', \"'s\", 'inexpensive', '.', 'Free-for-all', '.', \n",
      " |              'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nlptk.misc.mixins.TokenizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from nlptk.misc.mixins.LemmatizerMixin:\n",
      " |  \n",
      " |  lemmatize(tokens, lang, **kwargs)\n",
      " |  \n",
      " |  lemmatize_en(text, *args, **kwargs)\n",
      " |      возвращает слово вместе с частью речи\n",
      " |      работает через библиотеку pattern\n",
      " |  \n",
      " |  lemmatize_nltk(tokens, *args, lang='eng', **kwargs)\n",
      " |  \n",
      " |  lemmatize_pt(tokens, *args, lang='en', **kwargs)\n",
      " |  \n",
      " |  lemmatize_ru(tokens, lang='ru', **kwargs)\n",
      " |  \n",
      " |  lemmatize_uk(tokens, lang='uk', **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from nlptk.misc.mixins.SentencizerMixin:\n",
      " |  \n",
      " |  sentencize_nltk(text, *args, lang='english', **kwargs)\n",
      " |      сегментация текста на предложения\n",
      " |  \n",
      " |  sentencize_polyglot(text, *args, **kwargs)\n",
      " |      сегментация текста на предложения\n",
      " |  \n",
      " |  sentencize_razdel(text, *args, **kwargs)\n",
      " |      сегментация текста на предложения\n",
      " |  \n",
      " |  sentencize_segtok(text, *args, **kwargs)\n",
      " |      сегментация текста на предложения\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nlptk.misc.mixins.TaggerMixin:\n",
      " |  \n",
      " |  tagger4 = tag(tokens) from nltk.tag.sequential.NgramTagger\n",
      " |      Determine the most appropriate tag sequence for the given\n",
      " |      token sequence, and return a corresponding list of tagged\n",
      " |      tokens.  A tagged token is encoded as a tuple ``(token, tag)``.\n",
      " |      \n",
      " |      :rtype: list(tuple(str, str))\n",
      " |  \n",
      " |  tagger_3ngram(tokens, *args, lang='eng', **kwargs)\n",
      " |      3-gram tagger\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from nlptk.misc.mixins.TaggerMixin:\n",
      " |  \n",
      " |  tagger_4ngram(tokens, *args, lang='eng', **kwargs)\n",
      " |      4-gram tagger\n",
      " |  \n",
      " |  tagger_nltk(tokens, *args, lang='eng', **kwargs)\n",
      " |      Perceptron tagger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep = Prep()\n",
    "help(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Prep()\n",
    "rules_clean=OrderedDict(\n",
    "        roman_numerals=(True,)\n",
    ")\n",
    "    \n",
    "rules_filter=OrderedDict(\n",
    "        by_tagpos=(True,{},{\"FW\"}),\n",
    "        short=(True,3),\n",
    "        ifnotin_lexicon=True\n",
    "        #trailing_chars=True\n",
    ")\n",
    "clean = TextCleaner(rules_clean)\n",
    "filters = TokenFilter(rules_filter)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hyphenation', (True,)),\n",
       "             ('accent', (True,)),\n",
       "             ('tags', (True,)),\n",
       "             ('urls', (True,)),\n",
       "             ('numeric', (True,)),\n",
       "             ('nonletter_sequences', (True,)),\n",
       "             ('quotes', (True,)),\n",
       "             ('multiple_whitespaces', (True,)),\n",
       "             ('roman_numerals', (True,))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tfidf in module nlptk.mining.text:\n",
      "\n",
      "tfidf(n_doc, token=None, texts: List[List[str]] = [], sort=False) method of nlptk.mining.text.Corpus instance\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "source = os.path.abspath(os.path.join(MODULEDIR,r'corpus\\en'))\n",
    "corpus = Corpus(Path(source,\"*.txt\"), prep, clean, filters)\n",
    "corpus.verbose = True\n",
    "print(help(corpus.tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for text in corpus:    \n",
    "    for i,sent in enumerate(text):\n",
    "        pass\n",
    "        \n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedSentence('THE⁄DT⁄THE\n",
       "END⁄NN⁄END', n=6507)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(name='wilde_picture_of_dorian_gray_txt.txt', nsents=13016, nwords=158242, nlemmas=5861)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedSentence('THE⁄DT⁄THE\n",
       "PICTURE⁄NNP⁄PICTURE\n",
       "OF⁄NNP⁄OF\n",
       "DORIAN⁄NNP⁄DORIAN\n",
       "GRAY⁄NNP⁄GRAY\n",
       "BY⁄NNP⁄BY\n",
       "OSCAR⁄NNP⁄OSCAR\n",
       "WILDE⁄NNP⁄WILDE\n",
       "THE⁄NNP⁄THE\n",
       "PREFACE⁄NNP⁄PREFACE\n",
       "The⁄DT⁄The\n",
       "artist⁄NN⁄artist\n",
       "is⁄VBZ⁄be\n",
       "the⁄DT⁄the\n",
       "creator⁄NN⁄creator\n",
       "of⁄IN⁄of\n",
       "beautiful⁄JJ⁄beautiful\n",
       "things⁄NNS⁄thing', n=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Token(token='the', idx=0, pos='DT', lemma='the', nsent=0),\n",
       " Token(token='picture', idx=1, pos='NNP', lemma='picture', nsent=0),\n",
       " Token(token='of', idx=2, pos='NNP', lemma='of', nsent=0),\n",
       " Token(token='dorian', idx=3, pos='NNP', lemma='dorian', nsent=0),\n",
       " Token(token='gray', idx=4, pos='NNP', lemma='gray', nsent=0),\n",
       " Token(token='by', idx=5, pos='NNP', lemma='by', nsent=0),\n",
       " Token(token='oscar', idx=6, pos='NNP', lemma='oscar', nsent=0),\n",
       " Token(token='wilde', idx=7, pos='NNP', lemma='wilde', nsent=0),\n",
       " Token(token='the', idx=8, pos='NNP', lemma='the', nsent=0),\n",
       " Token(token='preface', idx=9, pos='NNP', lemma='preface', nsent=0),\n",
       " Token(token='the', idx=10, pos='DT', lemma='the', nsent=0),\n",
       " Token(token='artist', idx=11, pos='NN', lemma='artist', nsent=0),\n",
       " Token(token='is', idx=12, pos='VBZ', lemma='be', nsent=0),\n",
       " Token(token='the', idx=13, pos='DT', lemma='the', nsent=0),\n",
       " Token(token='creator', idx=14, pos='NN', lemma='creator', nsent=0),\n",
       " Token(token='of', idx=15, pos='IN', lemma='of', nsent=0),\n",
       " Token(token='beautiful', idx=16, pos='JJ', lemma='beautiful', nsent=0),\n",
       " Token(token='things', idx=17, pos='NNS', lemma='thing', nsent=0))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.sents[0].tokens(lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Token(token='picture', idx=1, pos='NNP', lemma='picture', nsent=0),\n",
       " Token(token='dorian', idx=3, pos='NNP', lemma='dorian', nsent=0),\n",
       " Token(token='gray', idx=4, pos='NNP', lemma='gray', nsent=0),\n",
       " Token(token='oscar', idx=6, pos='NNP', lemma='oscar', nsent=0),\n",
       " Token(token='wilde', idx=7, pos='NNP', lemma='wilde', nsent=0),\n",
       " Token(token='preface', idx=9, pos='NNP', lemma='preface', nsent=0),\n",
       " Token(token='artist', idx=11, pos='NN', lemma='artist', nsent=0),\n",
       " Token(token='creator', idx=14, pos='NN', lemma='creator', nsent=0),\n",
       " Token(token='beautiful', idx=16, pos='JJ', lemma='beautiful', nsent=0),\n",
       " Token(token='things', idx=17, pos='NNS', lemma='thing', nsent=0))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.sents[0].tokens(lower=True,filtrate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'dorian',\n",
       " 'gray',\n",
       " 'by',\n",
       " 'oscar',\n",
       " 'wilde',\n",
       " 'the',\n",
       " 'preface',\n",
       " 'the',\n",
       " 'artist',\n",
       " 'is',\n",
       " 'the',\n",
       " 'creator',\n",
       " 'of',\n",
       " 'beautiful',\n",
       " 'things')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.sents[0].words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'dorian',\n",
       " 'gray',\n",
       " 'by',\n",
       " 'oscar',\n",
       " 'wilde',\n",
       " 'the',\n",
       " 'preface',\n",
       " 'the',\n",
       " 'artist',\n",
       " 'be',\n",
       " 'the',\n",
       " 'creator',\n",
       " 'of',\n",
       " 'beautiful',\n",
       " 'thing')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.sents[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'dorian',\n",
       " 'gray',\n",
       " 'by',\n",
       " 'oscar',\n",
       " 'wilde',\n",
       " 'the',\n",
       " 'preface']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.words(filtrate=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'be': 7086, 'have': 3158, 'say': 776, 'do': 740, 'go': 590, 'know': 518, 'come': 460, 'look': 420, 'make': 410, 'think': 390, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs,cond = text.postags(\"VERB\")[:10]\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'dorian': 582, 'good': 276, 'own': 256, 'great': 164, 'little': 164, 'young': 156, 'other': 144, 'more': 130, 'such': 130, 'old': 124, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives,cond = text.postags(\"ADJ\")[:10]\n",
    "adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('picture', 'dorian', 'gray')\n",
      "('picture', 'dorian', 'oscar')\n",
      "('picture', 'dorian', 'wilde')\n",
      "('picture', 'gray', 'oscar')\n",
      "('picture', 'gray', 'wilde')\n",
      "('picture', 'oscar', 'wilde')\n",
      "('dorian', 'gray', 'oscar')\n",
      "('dorian', 'gray', 'wilde')\n",
      "('dorian', 'gray', 'preface')\n",
      "('dorian', 'oscar', 'wilde')\n",
      "('dorian', 'oscar', 'preface')\n",
      "('dorian', 'wilde', 'preface')\n"
     ]
    }
   ],
   "source": [
    "for n,s in enumerate(text.skipgrams(3,2)):\n",
    "    print(s)\n",
    "    if n > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('picture', 'dorian', 'gray')\n",
      "('dorian', 'gray', 'oscar')\n",
      "('gray', 'oscar', 'wilde')\n",
      "('oscar', 'wilde', 'preface')\n",
      "('wilde', 'preface', 'artist')\n",
      "('preface', 'artist', 'creator')\n",
      "('artist', 'creator', 'beautiful')\n",
      "('creator', 'beautiful', 'thing')\n",
      "('beautiful', 'thing', 'reveal')\n",
      "('thing', 'reveal', 'art')\n",
      "('reveal', 'art', 'conceal')\n",
      "('art', 'conceal', 'artist')\n"
     ]
    }
   ],
   "source": [
    "for n,s in enumerate(text.ngrams(3)):\n",
    "    print(s)\n",
    "    if n > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 401),\n",
       " (1, 557),\n",
       " (2, 375),\n",
       " (3, 486),\n",
       " (4, 1474),\n",
       " (5, 24),\n",
       " (6, None),\n",
       " (7, 2),\n",
       " (8, 234),\n",
       " (9, 51),\n",
       " (10, 390),\n",
       " (11, 430),\n",
       " (12, 70),\n",
       " (13, 341),\n",
       " (14, 1286),\n",
       " (15, 567),\n",
       " (16, 356),\n",
       " (17, 1451),\n",
       " (18, 531),\n",
       " (19, 166),\n",
       " (20, 262)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[(n,corpus.cfs(n).get('said',0)) for n in range(corpus.ndocs)]\n",
    "[(n,corpus.cfs(n,'said')) for n in range(corpus.ndocs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('darcy', 0.02173285494174185), ('elizabeth', 0.01881861805878887)],\n",
       "  'Austin Pride and Prejudice.txt'),\n",
       " (1,\n",
       "  [('rochester', 0.011592943365875099), ('jane', 0.007670881020111381)],\n",
       "  'bronte_jane_txt.txt'),\n",
       " (2,\n",
       "  [('heathcliff', 0.024757952256698943), ('cathy', 0.006880476042835574)],\n",
       "  'bronte_wuthering_txt.txt'),\n",
       " (3,\n",
       "  [('holmes', 0.031655249723820084), ('sherlock', 0.005133082773210995)],\n",
       "  'doyle_the_adventures.txt'),\n",
       " (4,\n",
       "  [('carrie', 0.05447190059425604), ('vance', 0.0033183040710108506)],\n",
       "  'dreiser_sister.txt'),\n",
       " (5,\n",
       "  [('amontillado', 0.048277858279063196), ('ugh', 0.018623830056918256)],\n",
       "  'Edgar Allan Poe The Cask of Amontillado.txt'),\n",
       " (6,\n",
       "  [('courtiers', 0.008605215945703487), ('waltzers', 0.008356420231628792)],\n",
       "  'Edgar Allan Poe The Masque of the Red Death.txt'),\n",
       " (7,\n",
       "  [('vulture', 0.0074646833560745324), ('louder', 0.0046287185990219765)],\n",
       "  'Edgar Allan Poe The Tell-Tale Heart.txt'),\n",
       " (8,\n",
       "  [('gatsby', 0.025526643271146482), ('daisy', 0.010829961402291284)],\n",
       "  'fitzgerald_great_gatsby_txt.txt'),\n",
       " (9,\n",
       "  [('gregor', 0.061835064820061354), ('grete', 0.006836037316790703)],\n",
       "  'Franz Kafka - Metamorphosis.txt'),\n",
       " (10,\n",
       "  [('lennie', 0.06675765917241691), ('gon', 0.02033535292201305)],\n",
       "  'John Steinbeck - Of Mice and Men.txt'),\n",
       " (11,\n",
       "  [('baloo', 0.012496074977993266), ('shere', 0.01156551620303632)],\n",
       "  'kipling_jungle_book_txt.txt'),\n",
       " (12,\n",
       "  [('fang', 0.03726068834880901), ('scott', 0.005225508644759031)],\n",
       "  'london_white_txt.txt'),\n",
       " (13,\n",
       "  [('jim', 0.005282021654753141), ('smollett', 0.004498998742202615)],\n",
       "  'stevenson_treasure_island_txt.txt'),\n",
       " (14,\n",
       "  [('corvallis', 0.011862941622458505), ('sophia', 0.008613991844776177)],\n",
       "  'Stivenson_Fall-or-Dodge-in-Hell_RuLit_Me.txt'),\n",
       " (15,\n",
       "  [('mina', 0.009850757160913439), ('jonathan', 0.0083001750152141)],\n",
       "  'stoker_dracula_txt.txt'),\n",
       " (16,\n",
       "  [('tom', 0.022654507611278674), ('huck', 0.020381131049581506)],\n",
       "  'twain_tom_sawyer_txt.txt'),\n",
       " (17,\n",
       "  [('knight', 0.013558048812582674), ('cedric', 0.01050599227164658)],\n",
       "  'walter_scott_ivanhoe_txt.txt'),\n",
       " (18,\n",
       "  [('kemp', 0.027741413382746796), ('bunting', 0.005600379227502875)],\n",
       "  'wells_invisible_man_txt.txt'),\n",
       " (19,\n",
       "  [('martians', 0.017919621957661603), ('martian', 0.008319824480342887)],\n",
       "  'wells_war_of_the_worlds_txt.txt'),\n",
       " (20,\n",
       "  [('dorian', 0.03547971877918586), ('basil', 0.010200760302427472)],\n",
       "  'wilde_picture_of_dorian_gray_txt.txt')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n,corpus.tfidf(n).get('said')) for n in range(corpus.ndocs)]\n",
    "[(n,corpus.tfidf(n,sort=-1)[0:2],\n",
    "        os.path.basename(path)\n",
    " ) \n",
    "     for n,path in zip(range(corpus.ndocs),Path(source,\"*.txt\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004578383282570111"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.tfidf(6,\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courtiers', 0.008605215945703487),\n",
       " ('waltzers', 0.008356420231628792),\n",
       " ('mummer', 0.008356420231628792),\n",
       " ('ebony', 0.006564888038834963),\n",
       " ('prince', 0.006109273652849401),\n",
       " ('musicians', 0.005341015962640384),\n",
       " ('revellers', 0.005341015962640384),\n",
       " ('suite', 0.004584676920385611),\n",
       " ('princes', 0.004551403686926438),\n",
       " ('sable', 0.004551403686926438)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.tfidf(6,sort=-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus.tfidf(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tfidf in module nlptk.mining.text:\n",
      "\n",
      "tfidf(n_doc, texts: List[List[str]] = None, token=None, sort=False) method of nlptk.mining.text.Corpus instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(corpus.tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "81937\n",
      "1\n",
      "0.003807803556390886\n",
      "3.044522437723423\n",
      "0.011592943365875099\n"
     ]
    }
   ],
   "source": [
    "print(corpus.cfs(1,'rochester'))\n",
    "print(sum(corpus.cfs(1).values()))\n",
    "print(corpus.dfs('rochester'))\n",
    "print(corpus.tf(1,'rochester'))\n",
    "print(corpus.idf('rochester'))\n",
    "print(corpus.tfidf(1,token='rochester'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--cfc--\n",
      "[('mr.', 783),\n",
      " ('elizabeth', 593),\n",
      " ('could', 524),\n",
      " ('would', 469),\n",
      " ('said', 401),\n",
      " ('darcy', 373),\n",
      " ('mrs.', 343),\n",
      " ('much', 327),\n",
      " ('bennet', 293),\n",
      " ('miss', 283)]\n",
      "--ccfc--\n",
      "[('said', 9454),\n",
      " ('one', 6043),\n",
      " ('would', 5993),\n",
      " ('could', 5226),\n",
      " ('time', 3394),\n",
      " ('like', 3303),\n",
      " ('little', 3174),\n",
      " ('well', 3038),\n",
      " ('see', 2983),\n",
      " ('know', 2745)]\n",
      "--dfc--\n",
      "[('long', 21),\n",
      " ('passed', 21),\n",
      " ('came', 21),\n",
      " ('lay', 21),\n",
      " ('looked', 21),\n",
      " ('four', 21),\n",
      " ('given', 21),\n",
      " ('back', 21),\n",
      " ('eyes', 21),\n",
      " ('might', 21),\n",
      " ('found', 21),\n",
      " ('yet', 21),\n",
      " ('life', 21),\n",
      " ('minutes', 21),\n",
      " ('felt', 21),\n",
      " ('three', 21),\n",
      " ('light', 21),\n",
      " ('heart', 21),\n",
      " ('turned', 21),\n",
      " ('time', 21),\n",
      " ('close', 21),\n",
      " ('one', 21),\n",
      " ('many', 21),\n",
      " ('moment', 21),\n",
      " ('threw', 21),\n",
      " ('like', 21),\n",
      " ('loud', 21),\n",
      " ('silence', 21),\n",
      " ('still', 21),\n",
      " ('tell', 21),\n",
      " ('seven', 21),\n",
      " ('told', 21),\n",
      " ('made', 21),\n",
      " ('thought', 21),\n",
      " ('voice', 21),\n",
      " ('hand', 21),\n",
      " ('suddenly', 21),\n",
      " ('without', 21),\n",
      " ('little', 21),\n",
      " ('come', 21),\n",
      " ('first', 21),\n",
      " ('could', 21),\n",
      " ('person', 21),\n",
      " ('new', 21),\n",
      " ('much', 21),\n",
      " ('well', 21),\n",
      " ('face', 21),\n",
      " ('stood', 21),\n",
      " ('even', 21),\n",
      " ('see', 21)]\n",
      "--tfidf--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input(\"--cfc--\")\n",
    "pprint(corpus.cfs(0,sort=-1)[:10])\n",
    "input(\"--ccfc--\")\n",
    "pprint(corpus.ccfs(sort=-1)[:10])\n",
    "input(\"--dfc--\")\n",
    "pprint(corpus.dfs(sort=-1)[:50])\n",
    "input(\"--tfidf--\")\n",
    "\n",
    "#for n in range(corpus.ndocs):\n",
    "#    pprint(corpus.tfidf(n,sort=-1)[:10]) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10711"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hapaxes = corpus.hapaxes()\n",
    "len(all_hapaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a.m',\n",
       " 'aah',\n",
       " 'abashing',\n",
       " 'abbeys',\n",
       " 'abbots',\n",
       " 'abbreviated',\n",
       " 'abdicate',\n",
       " 'abdomen',\n",
       " 'abduction',\n",
       " 'abe']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_hapaxes)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniting',\n",
       " 'city',\n",
       " 'pollution',\n",
       " 'liberties',\n",
       " 'sportive',\n",
       " 'arrear',\n",
       " 'heretofore',\n",
       " 'retain',\n",
       " 'bath',\n",
       " 'cheap']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapaxes = [(os.path.basename(path), corpus.hapaxes(n)) for n,path in zip(range(corpus.ndocs),Path(source,\"*.txt\"))]\n",
    "hapaxes[0][1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Austin Pride and Prejudice.txt', 2349),\n",
       " ('bronte_jane_txt.txt', 5065),\n",
       " ('bronte_wuthering_txt.txt', 3851),\n",
       " ('doyle_the_adventures.txt', 3401),\n",
       " ('dreiser_sister.txt', 4074),\n",
       " ('Edgar Allan Poe The Cask of Amontillado.txt', 511),\n",
       " ('Edgar Allan Poe The Masque of the Red Death.txt', 552),\n",
       " ('Edgar Allan Poe The Tell-Tale Heart.txt', 366),\n",
       " ('fitzgerald_great_gatsby_txt.txt', 2960),\n",
       " ('Franz Kafka - Metamorphosis.txt', 1277),\n",
       " ('John Steinbeck - Of Mice and Men.txt', 1203),\n",
       " ('kipling_jungle_book_txt.txt', 1896),\n",
       " ('london_white_txt.txt', 2847),\n",
       " ('stevenson_treasure_island_txt.txt', 2715),\n",
       " ('Stivenson_Fall-or-Dodge-in-Hell_RuLit_Me.txt', 6560),\n",
       " ('stoker_dracula_txt.txt', 3968),\n",
       " ('twain_tom_sawyer_txt.txt', 3358),\n",
       " ('walter_scott_ivanhoe_txt.txt', 4805),\n",
       " ('wells_invisible_man_txt.txt', 2729),\n",
       " ('wells_war_of_the_worlds_txt.txt', 3140),\n",
       " ('wilde_picture_of_dorian_gray_txt.txt', 3034)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(h[0],len(h[1])) for h in hapaxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.ccfs('imbecile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.cfs(0,'pollution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
